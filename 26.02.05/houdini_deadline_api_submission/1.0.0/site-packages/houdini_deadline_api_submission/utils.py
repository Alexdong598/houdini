"""Utility functions."""

# Import built-in modules
import copy
import logging
import os
import re
import time

# Import third-party modules
import hou  # pylint: disable=import-error

# Import local modules
from houdini_deadline_api_submission import constants


def get_last_index(array, item):
    """Return the last found index of item in array.

    Args:
        array (list):
        item: Item to search for.

    Returns:
        int: Index of the last given item in the array.

    """
    rev_array = copy.copy(array)
    rev_array.reverse()
    index = rev_array.index(item)
    index = len(array) - 1 - index
    return index


def get_frames(node):
    """Get the start, end and frame increment of a node.

    Args:
        node (hou.Node): Configured frames of this node will be returned.

    Returns:
        int, int, int: Start frame, end frame and frame increment.

    """
    mode = node.parm("trange")
    node_type = node.type().name()
    if mode:
        if mode.eval() == 0:  # Single Frame, use current frame.
            start, end, inc = (int(hou.frame()), int(hou.frame()), 1)
        else:
            start, end, inc = (
                int(node.evalParm("f1")),
                int(node.evalParm("f2")),
                max(float(node.evalParm("f3")), 1.0),
            )
    elif node_type == "fetch":
        start, end, inc = get_frames(node.parm("source").evalAsNode())
    else:
        children = node.children()
        start, end, inc = float("inf"), None, float("inf")
        for child in children:
            this_start, this_end, this_inc = get_frames(child)
            start = min(start, this_start)
            end = max(end, this_end)
            inc = min(inc, this_inc)

    return start, end, inc


def get_timestamp(delim=" "):
    """Get a human readable timestamp of the current time.

    Returns:
        str: Human readable timestamp of the current time.

    """
    return time.strftime("%Y{delim}%m{delim}%d{delim}%H{delim}%M".format(delim=delim))


def get_dependencies(base_node):
    """Return all dependencies node needed for the given node to render.

    Args:
        base_node (hou.Node): Node to get all the dependencies of. (Including
            itself)

    Returns:
        list of dict: Dependencies for the given node, in order. (Including
            itself)

    Examples:
        >>>get_dependencies(hou.node('/out/render_arnold'))
        [
            {
                'node': <hou.RopNode of type comp at /out/prepare/base_comp>,
                'dependencies': [],
            },
            {
                'node': <hou.RopNode of type geometry at /out/prepare/dependent_on_base_comp>,
                'dependencies': [
                    <hou.RopNode of type comp at /out/base_comp>
                ]
            },
            {
                'node': <hou.RopNode of type geometry at /out/dependent_on_nothing>,
                'dependencies': []
            },
            {
                'node': <hou.RopNode of type arnold at /out/render_arnold>,
                'dependencies': [
                    <hou.RopNode of type geometry at /out/dependent_on_base_comp>,
                    <hou.RopNode of type geometry at /out/dependent_on_nothing>
                ]
            }
        ]

    """
    # The hscript command `render` allows us to get a string representing all
    # dependencies and the frame range needed to render the base_node. This can
    # lead to some discrepancies when the base_node has a different frame range
    # than some of it's dependencies. Which is why we ignore the frame range
    # returned from this function.
    # Returned value has this format:
    # <<id>> [ <<dependencies>> ] <<node>> <<frames>>
    render_info = hou.hscript("render -p -c -F {}".format(base_node.path()))[0]

    tasks = render_info.split("\n")[:-1]
    dependencies = []

    for task_info in tasks:
        # The task_id index starts with 1, to make it compatible with Python
        # list indexes, we subtract 1 from it.
        task_node = re.search(r" (/\S+) ", task_info).group(1)
        task_dependencies = re.search(r" \[ ([0-9 ]*) ?\] ", task_info).group(1).split()
        task_dependencies = [int(i) - 1 for i in task_dependencies]
        task_dictionary = {
            "node": hou.node(task_node),
            "dependencies": task_dependencies,
        }
        dependencies.append(task_dictionary)

    # Update dependencies to hou.Node objects.
    for task_id, task_info in enumerate(dependencies):
        task_info["dependencies"] = [
            dependencies[i]["node"] for i in task_info["dependencies"]
        ]
        dependencies[task_id] = task_info
    return dependencies


def reduce_to_parents(dependencies):
    """Return a simplified list of tasks, reduced to their parent nodes.

    Args:
        dependencies (list of dict): Information gathered by
            `get_dependencies`.

    Returns:
        list of dict: Dependency entries reduced to their parents.

    """
    # Save orig node.
    for task_info in dependencies:
        task_info["orig_node"] = task_info["node"]

    # Correct for any fetch nodes inside parent folders.
    fetches = get_fetches(dependencies)
    dependencies = correct_fetches(dependencies, fetches)

    # First update all the node and dependency information to their parent,
    # if valid.
    for task_info in dependencies:
        node = task_info["node"]
        parent = find_parent(node)
        if parent:
            task_info["node"] = parent

        this_dependencies = task_info["dependencies"]
        this_dependencies = [find_parent(node) or node for node in this_dependencies]
        this_dependencies = list(set(this_dependencies))
        task_info["dependencies"] = this_dependencies

    # Since we're merging different ROPs which could have different
    # dependencies, we need to merge the dependencies together. We're using a
    # list of the ROPs to get the first index, which will then gather all the
    # dependencies.
    all_nodes = [task_info["node"] for task_info in dependencies]
    for task_info in dependencies:
        node = task_info["node"]
        if all_nodes.count(node) > 1:
            last_index = get_last_index(all_nodes, node)
            dependencies[last_index]["dependencies"] += task_info["dependencies"]
            dependencies[last_index]["dependencies"] = list(
                set(dependencies[last_index]["dependencies"])
            )

    # We only want to keep the first occurrence of each node, since we gathered
    # all the dependencies on this one.
    indices_to_use = [
        i
        for i, node in enumerate(all_nodes)
        if all_nodes.count(node) == 1 or i == get_last_index(all_nodes, node)
    ]
    dependencies = [dependencies[i] for i in indices_to_use]

    # In the first step, we converted all dependencies to their parent node.
    # This means as soon as there is one dependency in the children of this
    # parent, we will have it be dependent on itself, which can never work.
    # Therefore, we need to remove any self dependency.
    # We also make sure we only have dependencies that still exist, because
    # they might have been deleted while correcting the fetch nodes.
    all_nodes = [task_info["node"] for task_info in dependencies]
    for task_info in dependencies:
        task_info["dependencies"] = [
            n
            for n in task_info["dependencies"]
            if n != task_info["node"] and n in all_nodes
        ]

    # It's important to return the dependency_ids entry as well, since this
    # will be used to make sure the created job objects are dependent on each
    # other.
    for task_info in dependencies:
        task_info["dependency_ids"] = [
            all_nodes.index(node) for node in task_info["dependencies"]
        ]

    return dependencies


def get_fetches(dependency_tree):
    """Return all fetches inside the dependency tree.

    Args:
        dependency_tree (list of dict):

    Returns:
        dict: All fetched nodes and their respective Fetch ROP.

    """
    logger = logging.getLogger(__name__)
    dept_dict = {leaf["orig_node"]: leaf["dependencies"] for leaf in dependency_tree}

    # Get the fetched ROP.
    fetched_nodes_dict = {}
    for node, dependencies in dept_dict.items():
        for dep in dependencies:
            inputs = node.inputAncestors(follow_subnets=True)
            if dep not in inputs:
                orig_dep = dep
                while dep.parent():
                    dep = dep.parent()
                    if dep in inputs:
                        break
                else:
                    fetched_nodes_dict[orig_dep] = inputs
                    logger.debug(
                        "Input of %s is getting fetched: %s", node.path(), orig_dep
                    )

    # Get the fetch, that fetches the ROP.
    fetched_dict = {}
    for fetched_node, node_inputs in fetched_nodes_dict.items():
        fetches = [
            parm.node()
            for parm in fetched_node.parmsReferencingThis()
            if parm.node().type().name() == "fetch" and parm.name() == "source"
        ]
        for fetch in fetches:
            if fetch in node_inputs:
                fetched_dict[fetched_node.path()] = fetch

    return fetched_dict


def correct_fetches(dependencies, fetches):
    """Set the node in the gathered dependencies to their Fetch ROP if needed.

    Args:
        dependencies (list of dict): Node dependencies as acquired by
            `get_dependencies`.
        fetches (dict): Fetch pairs as acquired by `get_fetches`.

    Returns:
        dict: Dependencies with their respective Fetch Node applied if needed.

    """
    for task_info in dependencies:
        node = task_info["orig_node"]
        if node.path() in fetches:
            fetch_node = fetches[node.path()]
            # We only have to apply this change if the parent of the Fetch ROP
            # and the parent of the original node differ.
            if find_parent(fetch_node) != find_parent(node):
                task_info["node"] = fetches[node.path()]
    return dependencies


def get_dependency_tree(base_node):
    """Return the dependency tree going up from the given base_node.

    This also collapses Subnetwork ROPs and any other kind of ROP with children
    ROPs to one single entry, if it's configured to behave like this.

    Args:
        base_node (hou.Node): Node to get all the dependencies of. (Including
            itself)

    Returns:
        list of dict: Dependencies for the given node, in order. (Including
            itself)

    Examples:
        >>>get_dependency_tree(hou.node('/out/render_arnold'))
        [
            {
                'node': <hou.RopNode of type subnetwork at /out/prepare>,
                'dependencies': [],
                'dependency_ids': []
            },
            {
                'node': <hou.RopNode of type geometry at /out/dependent_on_nothing>,
                'dependencies': [],
                'dependency_ids': []
            },
            {
                'node': <hou.RopNode of type arnold at /out/render_arnold>,
                'dependencies': [
                    <hou.RopNode of type geometry at /out/dependent_on_base_comp>,
                    <hou.RopNode of type geometry at /out/dependent_on_nothing>
                ],
                'dependency_ids': [0, 1]
            }
        ]

    """
    dependencies = get_dependencies(base_node)
    dependencies = reduce_to_parents(dependencies)
    return dependencies


def log_message(node, message, indent=0, log_parm_name=constants.LOG_PARM):
    """Log a message onto a node.

    Args:
        node (hou.Node): Node to create the log on.
        message (str): Log message.
        indent (int, optional): Indentation of the logged message.
        log_parm_name (str, optional): Name of the logging parameter. Defaults
            to constants.LOG_PARM.

    """
    log_parm = node.parm(log_parm_name)
    if not log_parm:
        add_log_parms(node, log_parm_name)
        log_parm = node.parm(log_parm_name)

    if indent < 0:
        raise ValueError(
            "Indent can not be smaller than 0.\n" "Provided indent: {}".format(indent)
        )
    if indent > 0:
        str_indent = "{} ".format(">" * indent)
        message_lines = message.split("\n")
        message_lines = ["{}{}".format(str_indent, msg) for msg in message_lines]
        message = "\n".join(message_lines)

    # If there is no entry yet, we don't need a line break between entries.
    if log_parm.evalAsString():
        message = "\n{}".format(message)

    log_parm.set("{}{}".format(log_parm.evalAsString(), message))


def add_log_parms(node, log_parm_name=constants.LOG_PARM):
    """Add logging parameters to the node.

    Args:
        node (hou.Node): Node to add the parameters to.
        log_parm_name (str, optional): Name of the logging parameter. Defaults
            to constants.LOG_PARM.

    """
    parm_template_grp = node.parmTemplateGroup()
    log_main_folder = hou.FolderParmTemplate(
        "hal_submission_log_folder",
        constants.LOG_FOLDER,
        folder_type=hou.folderType.Tabs,
    )
    log_parm = hou.StringParmTemplate(
        log_parm_name,
        "HAL Submission Log",
        1,
    )
    # Display 20 lines and 40 columns.
    log_parm.setTags({"editor": "1", "editorlines": "20-40"})

    log_main_folder.addParmTemplate(log_parm)
    parm_template_grp.append(log_main_folder)
    node.setParmTemplateGroup(parm_template_grp)


def save_copy(folder):
    """Save a copy of the current hip file to the given folder.

    The created file will include a timestamp in it's name.

    Args:
        folder (str): Folder in which to save the hipfile to.

    """
    if not os.path.isdir(folder):
        os.makedirs(folder)

    orig_hip = hou.hipFile.path()
    hip, ext = os.path.splitext(os.path.basename(orig_hip))
    suffix = get_timestamp("_")
    if os.environ.get("WEDGE"):
        suffix = "{}__{}".format(suffix, os.environ["WEDGE"])
    hip = "{}__{}".format(suffix, hip)
    hip = "{}{}".format(hip, ext)
    path = os.path.join(folder, hip)
    hou.hipFile.save(path, save_to_recent_files=False)
    hou.hipFile.setName(orig_hip)
    return path


def is_pre_pass(node):
    """Return if the given node should be rendered as a PrePass.

    Args:
        node (hou.Node): Node of interest.

    Returns:
        bool: True if given node is a PrePass.

    """
    parm = node.parm(constants.IS_PRE_PASS)
    if not parm:
        return False
    return parm.eval()


def get_run_as_one(node):
    """Return the value of `constants.SUBMIT_CHILDREN_SEPARATELY` of this node.

    If this parameter doesn't exist, 'Driver' type nodes will default to True,
    while any other kind of node defaults to False.

    Args:
        node (hou.Node): Node to query the parm from.

    Returns:
        bool: If this node is configured to run a combined job.

    """
    parm = node.parm(constants.SUBMIT_CHILDREN_SEPARATELY)
    if parm:
        # The name changed from `hal_run_as_one_job` to
        # `hal_submit_children_separately`, hence the weird inversion of the
        # return value.
        return not bool(parm.eval())

    # We default to render Driver parent nodes as one job.
    node_type = node.type().category().name()
    return node_type == "Driver"


def find_parent(node):
    """Return the parent node of a ROP that is set to run a combined job.

    This is based on the parameter named in
    `constants.SUBMIT_CHILDREN_SEPARATELY`. If this parameter doesn't exist,
    'Driver' type nodes will default to being a parent, while any other kind of
    node defaults to not being a parent.

    Args:
        node (hou.Node): The search starts from this node upwards.

    Returns:
        hou.Node: The parent ROP that is set to run a combined job.

    """
    found_node = None
    # When the Node is configured to be run as a PrePass, we don't want it to
    # be simplified to a single job.
    if is_pre_pass(node):
        return None

    while node.parent():
        node = node.parent()
        if get_run_as_one(node):
            found_node = node

    return found_node
