"""Collection of special render Deadline jobs."""

# Import built-in modules
from abc import ABCMeta
from abc import abstractmethod
import os

# Import third-party modules
import hou
# from hal_ontrack.apps.houdini_funcs import track_outputs

# Import local modules
from houdini_deadline_api_submission import deadline_utils
from houdini_deadline_api_submission import naming
from houdini_deadline_api_submission import utils
from houdini_deadline_api_submission import constants
from houdini_deadline_api_submission.job.base import BaseDeadlineJob
from houdini_deadline_api_submission.job.utility import CleanUpJob


class CacheRenderDeadlineJob(BaseDeadlineJob):
    """Class for custom render ROP implementations."""

    __metaclass__ = ABCMeta

    def pre_submit_all(self):
        """Adjust jobs to include the render and/or cache jobs.

        Raises:
            IOError: If the node fails to return a render mode.

        """
        super(CacheRenderDeadlineJob, self).pre_submit_all()
        if naming.is_cache_and_render(self.node):
            self.jobs = self.split_into_cache_render_jobs(self.jobs)
        elif naming.is_only_cache(self.node):
            self.jobs = self.get_cache_jobs(self.jobs)
        elif naming.is_only_render(self.node):
            self.jobs = self.jobs
        else:
            raise IOError("Can't figure out what mode to choose.")

    def split_into_cache_render_jobs(self, job_list, create_cleanup=True):
        """Split each job into it's cache (.ifd, .ass, ...) and render job.

        Args:
            job_list (:obj:`list` of :obj:`houdini_deadline_api_submission.job.
                base.BaseDeadlineJob`): List of currently stored jobs.
            create_cleanup (bool, optional): Create cleanup jobs for the render
                caches. Defaults to True.

        Returns:
            :obj:`list` of :obj:`BaseDeadlineJob`: Cache creation and render
                jobs.

        """
        all_ifd_jobs = self.get_cache_jobs(job_list)
        all_render_jobs = self.get_render_jobs(job_list)

        # Create cleanup jobs if needed. Otherwise, we simply fill the list
        # with None, so the zip still works.
        if create_cleanup:
            all_cleanup_jobs = [CleanUpJob.from_job(job) for job in all_ifd_jobs]
        else:
            all_cleanup_jobs = [None for _ in all_ifd_jobs]

        for cache_job, render_job, cleanup_job in zip(
            all_ifd_jobs, all_render_jobs, all_cleanup_jobs
        ):
            render_job.add_dependency(cache_job)
            if cleanup_job:
                cleanup_job.add_dependency(render_job)

        if create_cleanup:
            return all_ifd_jobs + all_render_jobs + all_cleanup_jobs
        return all_ifd_jobs + all_render_jobs

    @abstractmethod
    def get_cache_jobs(self, job_list):  # pylint:disable=unused-argument
        """Return the render cache generation job for each job.

        This method is designed to be overwritten by another class.

        Args:
            job_list (:obj:`list` of :obj:`houdini_deadline_api_submission.job.
                base.BaseDeadlineJob`): List of currently stored jobs.

        Raises:
            NotImplemented: If the class based on this class does not override
                this method.

        Returns:
            :obj:`list` of :obj:`BaseDeadlineJob`: Cache creation jobs.

        """
        raise NotImplementedError(
            "Implement a method in a class that "
            "inherits from this class and overrides "
            "this method."
        )

    @abstractmethod
    def get_render_jobs(self, job_list):  # pylint:disable=unused-argument
        """Return the render job for each job.

        This method is designed to be overwritten by another class.

        Args:
            job_list (:obj:`list` of :obj:`houdini_deadline_api_submission.job.
                base.BaseDeadlineJob`): List of currently stored jobs.

        Raises:
            NotImplemented: If the class based on this class does not override
                this method.

        Returns:
            :obj:`list` of :obj:`BaseDeadlineJob`: Render jobs.

        """
        raise NotImplementedError(
            "Implement a method in a class that "
            "inherits from this class and overrides "
            "this method."
        )


class MantraDeadlineJob(CacheRenderDeadlineJob):
    """Special implementation for Mantra ROPs."""

    def get_cache_jobs(self, job_list):
        """Return BaseDeadlineJob's used to create the .ifd caches.

        Args:
            job_list (:obj:`list` of :obj:`houdini_deadline_api_submission.job.
                base.BaseDeadlineJob`): List of currently stored jobs.

        Returns:
            :obj:`list` of :obj:`BaseDeadlineJob`: Cache creation jobs for
                Mantra's IFD's.

        """
        cache_parm_states = {"soho_outputmode": True}
        cache_jobs = []
        for job in job_list:
            new_job = BaseDeadlineJob(
                job.node,
                job.dependencies,
                job.submitter_node,
                constants.HOUDINI_PLUGIN,
                job.job_info,
                job.plugin_info,
                job.wedge_values,
                job.wedge_index,
                "soho_diskfile",
                node_parm_overrides=cache_parm_states,
                timestamp=job.timestamp,
            )
            suffix = "IFD"
            if naming.is_only_cache(job.node):
                suffix = "IFD Cache"
            new_job.job_info["Name"] = "{} - {}".format(
                new_job.job_info["Name"], suffix
            )
            new_job.job_info["LimitGroups"] = "hbatch_render"
            new_job.job_info["ChunkSize"] = max(10, new_job.job_info["ChunkSize"])

            # if self.tracker:
            #     sgids = track_outputs(self.tracker, job.node, is_cache=True)
            #     deadline_utils.append_to_deadline_env(
            #         new_job.job_info,
            #         "_HAL_PUBLISHED_FILE_IDS",
            #         ",".join([str(int_) for int_ in sgids]),
            #     )

            cache_jobs.append(new_job)
        return cache_jobs

    def get_render_jobs(self, job_list):
        """Return BaseDeadlineJob's used to render .ifd files.

        Args:
            job_list (:obj:`list` of :obj:`houdini_deadline_api_submission.job.
                base.BaseDeadlineJob`): List of currently stored jobs.

        Returns:
            :obj:`list` of :obj:`BaseDeadlineJob`:  Mantra render jobs.

        """
        new_jobs = []
        for job in job_list:
            plugin_values = {
                # Passing a function so the value is correct when wedges are affecting
                # this parameter.
                "SceneFile": naming.get_output_parm(
                    job.node, "soho_diskfile"
                ).evalAsString,
                "Version": job.plugin_info["Version"],
                "Build": job.plugin_info["Build"],
                "CommandLineOptions": "",
                "Threads": 0,
            }

            new_job = BaseDeadlineJob(
                job.node,
                job.dependencies,
                job.submitter_node,
                constants.MANTRA_PLUGIN,
                job.job_info,
                plugin_values,
                job.wedge_values,
                job.wedge_index,
                "vm_picture",
                save_copy=False,
                timestamp=job.timestamp,
            )
            new_job.job_info["IsFrameDependent"] = True
            new_job.job_info["ChunkSize"] = 1
            new_job.job_info["Name"] = "{} - Mantra Render".format(
                new_job.job_info["Name"]
            )
            new_job.job_info["LimitGroups"] = "houdini_render"

            # if self.tracker:
            #     sgids = track_outputs(self.tracker, job.node)
            #     deadline_utils.append_to_deadline_env(
            #         new_job.job_info,
            #         "_HAL_PUBLISHED_FILE_IDS",
            #         ",".join([str(int_) for int_ in sgids]),
            #     )

            new_jobs.append(new_job)
        return new_jobs


class ArnoldDeadlineJob(CacheRenderDeadlineJob):
    """Special implementation for Arnold ROPs."""

    def get_cache_jobs(self, job_list):
        """Return BaseDeadlineJob's used to create the .ass caches.

        Args:
            job_list (:obj:`list` of :obj:`houdini_deadline_api_submission.job.
                base.BaseDeadlineJob`): List of currently stored jobs.

        Returns:
            :obj:`list` of :obj:`BaseDeadlineJob`: Cache creation jobs.

        """
        parm_overrides = {"ar_ass_export_enable": True}
        cache_jobs = []
        for job in job_list:
            new_job = BaseDeadlineJob(
                job.node,
                job.dependencies,
                job.submitter_node,
                constants.HOUDINI_PLUGIN,
                job.job_info,
                job.plugin_info,
                job.wedge_values,
                job.wedge_index,
                "ar_ass_file",
                node_parm_overrides=parm_overrides,
                timestamp=job.timestamp,
            )
            new_job.job_info["Name"] = "{} - ASS".format(new_job.job_info["Name"])
            new_job.job_info["LimitGroups"] = "hbatch_render"
            new_job.job_info["ChunkSize"] = max(10, new_job.job_info["ChunkSize"])

            # if self.tracker:
            #     sgids = track_outputs(self.tracker, job.node, is_cache=True)
            #     deadline_utils.append_to_deadline_env(
            #         new_job.job_info,
            #         "_HAL_PUBLISHED_FILE_IDS",
            #         ",".join([str(int_) for int_ in sgids]),
            #     )

            cache_jobs.append(new_job)
        return cache_jobs

    def get_render_jobs(self, job_list):
        """Return BaseDeadlineJob's used to render .ass files.

        Args:
            job_list (:obj:`list` of :obj:`houdini_deadline_api_submission.job.
                base.BaseDeadlineJob`): List of currently stored jobs.

        Returns:
            :obj:`list` of :obj:`BaseDeadlineJob`: Arnold render jobs.

        """
        new_jobs = []
        for job in job_list:
            plugin_values = {
                # Passing a function so the value is correct when wedges are affecting
                # this parameter.
                "InputFile": naming.get_output_parm(
                    job.node, "ar_ass_file"
                ).evalAsString,
                "Version": job.plugin_info["Version"],
                "Build": job.plugin_info["Build"],
                "CommandLineOptions": "",
                "Threads": 0,
            }

            new_job = BaseDeadlineJob(
                job.node,
                job.dependencies,
                job.submitter_node,
                constants.ARNOLD_PLUGIN,
                job.job_info,
                plugin_values,
                job.wedge_values,
                job.wedge_index,
                "ar_picture",
                save_copy=True,
                timestamp=job.timestamp,
            )
            new_job.job_info["IsFrameDependent"] = True
            new_job.job_info["ChunkSize"] = 1
            new_job.job_info["Name"] = "{} - Arnold Render".format(
                new_job.job_info["Name"]
            )
            new_job.job_info["LimitGroups"] = "arnold_render"

            # if self.tracker:
            #     sgids = track_outputs(self.tracker, job.node)
            #     deadline_utils.append_to_deadline_env(
            #         new_job.job_info,
            #         "_HAL_PUBLISHED_FILE_IDS",
            #         ",".join([str(int_) for int_ in sgids]),
            #     )

            new_jobs.append(new_job)
        return new_jobs


class FetchDeadlineJob(BaseDeadlineJob):
    """Special implementation for Fetch ROPs and USD Render ROPs.
    
    This class handles fetch nodes and usdrender_rop nodes by analyzing their source to determine the
    appropriate submission method.
    """
    def pre_submit_all(self):
        """Analyze the node's source and adjust jobs accordingly.
        
        This method analyzes the source of the node and determines the
        appropriate submission method.
        """
        super(FetchDeadlineJob, self).pre_submit_all()

        if self.node.type().name() == "usdrender_rop":
            self.source_node = self.node
        else:
            self.source_node = self.node.parm("source").evalAsNode()

        if hou.hipFile.hasUnsavedChanges():
            if hou.ui.displayMessage( "The scene has unsaved changes and must be saved before the job can be submitted.\nDo you wish to save?", buttons=( "Yes" , "No" ), title="Submit Houdini To Deadline" ) == 0:
                hou.hipFile.save()

        scene_file_path = hou.hipFile.path()
        # scene_file_path = os.path.dirname(hou.hipFile.path())
        # deadline_dir = os.path.join(scene_file_path, "_deadline")
        # os.makedirs(deadline_dir, exist_ok=True)
        # scene_file_path = os.path.join(deadline_dir, self.source_node.name())
        # scene_file_path = utils.save_copy(scene_file_path)

        new_jobs = []
        for job in self.jobs:
            plugin_values = {
                "SceneFile": scene_file_path,
            }

            new_job = BaseDeadlineJob(
                self.source_node,
                job.dependencies,
                job.submitter_node,
                constants.HOUDINI_PLUGIN,
                job.job_info,
                plugin_values,
                job.wedge_values,
                job.wedge_index,
                "outputimage",
                save_copy=False,
                timestamp=job.timestamp,
                tracker=job.tracker
            )

            new_jobs.append(new_job)

        self.jobs = new_jobs
